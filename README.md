<div align="center">
    
# ğŸ­ Masked Diffusion Language Models <br> with Frequency-Informed Training

### â­ Challenge Award at BabyLM Workshop @ EMNLP 2025

[**ğŸ“„ Paper**](https://arxiv.org/abs/2509.05056) â€¢ [**ğŸ¤— Models**](https://huggingface.co/despoinakk) â€¢ [**ğŸ’» Code**](https://github.com/DespoinaKK/babylm-diffusion)

</div>

---

Official PyTorch implementation of **"Masked Diffusion Language Models with Frequency-Informed Training"**

## ğŸŒŸ Overview

This repository implements a masked diffusion language modeling framework for data-efficient training under strict data constraints. Our approach combines:

- **Masked Diffusion Language Models (MDLMs)**: A discrete diffusion approach that uses masked language modelling (bidirectional context) to generate text
- **Novel Noise Schedules**: Bimodal Gaussian (our best model) and cosine (submission's model)
- **Frequency-Informed Masking**: Progressive prioritization (curriculm) of rare tokens during training, which integrates seamlessly with the MDLM framework
- **NELBO Reweighting**: Exploration of different weighting schemes to optimize performance across schedules

![Noise Schedule Evolution](assets/noise_schedule_evolution.gif)


### ğŸ¯ Results
Our method achieves competitive performance with state-of-the-art baselines (GPT-BERT) on the BabyLM benchmarks, demonstrating that diffusion-based training offers a viable alternative for data-restricted language learning.

## ğŸš€ Installation
```bash
# Clone the repository
git clone https://github.com/DespoinaKK/babylm-diffusion
cd babylm-diffusion

# Install dependencies
pip install -r requirements.txt
```

## âš¡ Quick Start

### 1. Data Preparation

First, train a BPE tokenizer on your corpus:
```bash
python tokenization/create_tokenizer.py \
    --input_path /path/to/data \
    --vocab_size 16384 \
    --vocab_path ./tokenizers/tokenizer.json
```

Then tokenize your dataset:
```bash
python tokenization/tokenize_corpus.py \
    --data_folder /path/to/data \
    --train_file data.train \
    --valid_file data.valid \
    --tokenizer_folder ./tokenizers \
    --tokenizer_file tokenizer.json \
    --name tokenized
```

### 2. Training

For distributed training, adapt the scripts in `slurm-scripts`.

## ğŸ”‘ Key Components

### ğŸŒŠ Noise Schedules

**Cosine Schedule** (`masking/noise_schedules.py`):
- Focuses on lower masking rates
- Average masking rate: 0.36

**Gaussian Schedules**:
- **Unimodal**
- **Bimodal**: Mixture distribution combining low and high masking modes
- Requires derivative softening (Î³ < 1.0) for stable training


### ğŸ“Š Frequency-Informed Masking

The frequency-informed masking strategy assigns higher masking probabilities to rare tokens. Implementation in `masking/frequency_masking.py`:

1. **Token Ranking**: Tokens ranked by global corpus frequency
2. **Min-Max Normalization**: Per-sequence normalization to [0,1]
3. **Softening**: Weights raised to power p < 1 to prevent over-emphasis on extremely rare tokens
4. **Conditional Scaling**: Ensures mean masking probability matches target rate (1 - Î±_t)

Optional curriculum learning progressively increases p from 0 to 0.02 across epochs.


## ğŸ“ˆ Evaluation

Models are evaluated using the [BabyLM Challenge evaluation pipeline](https://github.com/babylm/evaluation-pipeline-2025/) with MLM pseudo-likelihood backend:

- **Zero-shot**: BLiMP, BLiMP Supplement, EWoK, Entity Tracking, COMPS
- **Finetuning**: GLUE and SuperGLUE subsets. For finetuning, we provide the `eval-utils/classifier.py` helper file, with minor tensor shape changes to match our models. 
- **Human-likeness**: Reading task, Age of Acquisition, morphology tasks correlation with human performance

Evaluation can be run with or without time conditioning (see paper Section 3.2).


## ğŸ¯ Updated Results - Bimodal Gaussian Schedule
See paper for full results and ablations.
We also include our new 512-seq-len model's results, trained with the Gaussian Bimodal noise schedule described in the paper:

Performance comparison on BabyLM Challenge zero-shot tasks:

| Task | Baseline <br> (GPT-BERT)  | Submission <br> (Cosine) | **Best <br> (Bimodal Gaussian)** |
|------|-------------------|---------------------|-------------------------------|
| BLiMP | 80.5 | 76.9 | 78.2 |
| BLiMP Supplement | 73.0 | 72.4 |73.6 |
| EWoK | 52.4 | 51.8 | 52.5 |
| COMPS | 59.7 | 56.4 | 56.6 |
| Entity Tracking | 39.9 | 40.8 | 39.7 |

## ğŸ¤— Hugginface Pretrained Models

| Model | Downloads |
|-------|-----------|
| [Cosine - Submission](https://huggingface.co/despoinakk/diffusion_cosine_babylm) | ![](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/despoinakk/diffusion_cosine_babylm&query=$.downloads&label=&color=green) |
| [Bimodal Gaussian - Best](https://huggingface.co/despoinakk/diffusion_gaussian_babylm) | ![](https://img.shields.io/badge/dynamic/json?url=https://huggingface.co/api/models/despoinakk/diffusion_gaussian_babylm&query=$.downloads&label=&color=green) |


## ğŸ“ Citation

If you find this work useful for your research, please consider citing our paper:

```bibtex
@article{kosmopoulou2025masked,
  title={Masked Diffusion Language Models with Frequency-Informed Training},
  author={Kosmopoulou, Despoina and Georgiou, Efthymios and Dorovatas, Vaggelis and Paraskevopoulos, Georgios and Potamianos, Alexandros},
  journal={arXiv preprint arXiv:2509.05056},
  year={2025}
}
```

## ğŸ“š References

This repo is based on work from:
- [Simple and Effective Masked Diffusion Language Models](https://arxiv.org/abs/2406.07524)
- [GPT or BERT: why not both?](https://aclanthology.org/2024.conll-babylm.23/) 
- Architecture adapted from [LTG-BERT](https://arxiv.org/abs/2303.09859)


## ğŸ“ Code Structure
```
.
â”œâ”€â”€ main.py                      # Main training entry point
â”œâ”€â”€ model.py                     # Transformer model with diffusion
â”œâ”€â”€ config/                      # Configuration and arguments
â”‚   â”œâ”€â”€ arguments.py            # CLI argument parsing
â”‚   â””â”€â”€ model_configuration.py  # Model architecture config
â”œâ”€â”€ data/                        # Data loading and processing
â”‚   â”œâ”€â”€ dataset.py              # Dataset implementation
â”‚   â”œâ”€â”€ dataset_manager.py      # Manage Datasets (loading)
â”‚   â””â”€â”€ dataset_utils.py        # Utilities for data
â”œâ”€â”€ eval-utils
â”‚   â””â”€â”€ classifier_model.py      # file to use for finetuning
â”œâ”€â”€ masking/                     # Masking strategies
â”‚   â”œâ”€â”€ noise_schedules.py      # Diffusion noise schedules
â”‚   â”œâ”€â”€ masking_processor.py    # Token masking logic
â”‚   â”œâ”€â”€ frequency_masking.py    # Frequency-informed masking
â”‚   â””â”€â”€ batch_processing.py     # Batch preparation
â”œâ”€â”€ training/                    # Training infrastructure
â”‚   â”œâ”€â”€ training_loop.py        # Main training loop
â”‚   â”œâ”€â”€ ema.py                  # Exponential moving average
â”‚   â”œâ”€â”€ checkpoint_manager.py   # Checkpoint saving
â”‚   â”œâ”€â”€ validation.py           # Validation during training
â”‚   â”œâ”€â”€ model_setup.py          # Model loading and optimizer setup
â”‚   â””â”€â”€ distributed_setup.py    # DDP setup
â”œâ”€â”€ tokenization/                # Tokenization scripts
â”‚   â”œâ”€â”€ create_tokenizer.py     # Train BPE tokenizer
â”‚   â””â”€â”€ tokenize_corpus.py      # Tokenize datasets
â”œâ”€â”€ optimization/                # Optimizers
â”‚   â””â”€â”€ lamb.py                 # LAMB optimizer
â””â”€â”€ slurm-scripts/               # Scripts
    â”œâ”€â”€ slurm-train.sh          # SLURM job script
    â”œâ”€â”€ launch-train.sh         # Local launch script
    â”œâ”€â”€ config-cosine.json      # Cosine schedule config example 
    â””â”€â”€ config-gauss.json       # Gaussian schedule config example
```



## ğŸ“§ Contact

For questions or issues, please open a GitHub issue or contact:
- [Despoina](https://scholar.google.com/citations?user=roxd-tsAAAAJ&hl=en&oi=sra) | [github](https://github.com/DespoinaKK) | despoinakkosmopoulou[at]gmail[dot]com
- [Efthymis](https://scholar.google.com/citations?user=5Sc6GvEAAAAJ&hl=en) | [github](https://github.com/efthymisgeo) | efthymios[dot]georgiou[at]unibe[dot]ch
